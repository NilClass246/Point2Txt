{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bca2cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "import yaml\n",
    "from typing import List, Tuple\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117db0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PointBERT config from models/pointbert\\PointTransformer_8192point_2layer.yaml.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 00:58:45,599 - Transformer - INFO - PointBERT's weights are successfully loaded from models/pointbert/point_bert_v1.2.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6 dim of points.\n",
      "Using 384 output dim of points from PointBERT.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PointTransformer(\n",
       "  (group_divider): Group()\n",
       "  (encoder): Encoder(\n",
       "    (first_conv): Sequential(\n",
       "      (0): Conv1d(6, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (second_conv): Sequential(\n",
       "      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (reduce_dim): Linear(in_features=256, out_features=384, bias=True)\n",
       "  (pos_embed): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=128, out_features=384, bias=True)\n",
       "  )\n",
       "  (blocks): TransformerEncoder(\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.009)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.018)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.027)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.036)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.045)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.055)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.064)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.073)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.082)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.091)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_path): DropPath(drop_prob=0.100)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.pointbert.point_encoder import PointTransformer\n",
    "\n",
    "def cfg_from_yaml_file(cfg_file):\n",
    "    config = EasyDict()\n",
    "    with open(cfg_file, 'r') as f:\n",
    "        new_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    merge_new_config(config=config, new_config=new_config)\n",
    "    return config\n",
    "\n",
    "def merge_new_config(config, new_config):\n",
    "    for key, val in new_config.items():\n",
    "        if not isinstance(val, dict):\n",
    "            if key == '_base_':\n",
    "                with open(new_config['_base_'], 'r') as f:\n",
    "                    try:\n",
    "                        val = yaml.load(f, Loader=yaml.FullLoader)\n",
    "                    except:\n",
    "                        val = yaml.load(f)\n",
    "                config[key] = EasyDict()\n",
    "                merge_new_config(config[key], val)\n",
    "            else:\n",
    "                config[key] = val\n",
    "                continue\n",
    "        if key not in config:\n",
    "            config[key] = EasyDict()\n",
    "        merge_new_config(config[key], val)\n",
    "    return config\n",
    "\n",
    "point_bert_config_name = \"PointTransformer_8192point_2layer\"\n",
    "point_bert_config_addr = os.path.join(\"models/pointbert\", f\"{point_bert_config_name}.yaml\")\n",
    "print(f\"Loading PointBERT config from {point_bert_config_addr}.\")\n",
    "point_bert_config = cfg_from_yaml_file(point_bert_config_addr)\n",
    "\n",
    "point_bert_config.model.point_dims = 6  # Use 6D points (XYZ + RGB)\n",
    "use_max_pool = False\n",
    "point_encoder = PointTransformer(point_bert_config.model, use_max_pool=use_max_pool).to(device)\n",
    "print(f\"Using {point_encoder.point_dims} dim of points.\")\n",
    "\n",
    "point_encoder.load_checkpoint(\"models/pointbert/point_bert_v1.2.pt\")\n",
    "\n",
    "backbone_output_dim = point_bert_config.model.trans_dim\n",
    "print(f\"Using {backbone_output_dim} output dim of points from PointBERT.\")\n",
    "\n",
    "# freeze PointBERT parameters\n",
    "for param in point_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "point_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3102fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 vocab size: 50257\n",
      "GPT-2 hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default; set pad = eos\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt2.resize_token_embeddings(len(tokenizer))\n",
    "gpt2.to(device)\n",
    "\n",
    "print(\"GPT-2 vocab size:\", len(tokenizer))\n",
    "print(\"GPT-2 hidden size:\", gpt2.config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c27f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "class PointBertClipCap3D(nn.Module):\n",
    "    \"\"\"\n",
    "    CLIPCap-style model:\n",
    "        point cloud -> point encoder (Point-BERT) -> MLP mapper -> GPT-2 prefix\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self, point_encoder: nn.Module, gpt2: GPT2LMHeadModel, prefix_len: int = 10):\n",
    "        super().__init__()\n",
    "        self.point_encoder = point_encoder\n",
    "        self.gpt2 = gpt2\n",
    "        self.prefix_len = prefix_len\n",
    "\n",
    "        point_emb_dim = backbone_output_dim\n",
    "        gpt_emb_dim = gpt2.config.n_embd\n",
    "\n",
    "        # Map global point embedding -> (prefix_len * gpt_emb_dim)\n",
    "        self.mapper = nn.Sequential(\n",
    "            nn.Linear(point_emb_dim, gpt_emb_dim * prefix_len),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def encode_prefix(self, pts: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        pts: (B, N, C)\n",
    "        Returns:\n",
    "            prefix: (B, prefix_len, gpt_emb_dim)\n",
    "        \"\"\"\n",
    "        B = pts.size(0)\n",
    "        feats = self.point_encoder(pts)  # (B, D)\n",
    "        # print(global_feat.shape)\n",
    "        global_feat = feats.mean(dim=1)\n",
    "        mapped = self.mapper(global_feat)      # (B, prefix_len * H)\n",
    "        prefix = mapped.view(B, self.prefix_len, self.gpt2.config.n_embd)\n",
    "        return prefix\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        pts: torch.Tensor,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        labels: torch.Tensor = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        pts: (B, N, C)\n",
    "        input_ids: (B, T)\n",
    "        attention_mask: (B, T), optional\n",
    "        labels: (B, T), optional\n",
    "        \"\"\"\n",
    "\n",
    "        B = pts.size(0)\n",
    "        prefix = self.encode_prefix(pts)  # (B, prefix_len, gpt_emb_dim)\n",
    "\n",
    "        # Token embeddings from GPT-2\n",
    "        token_embeds = self.gpt2.transformer.wte(input_ids)  # (B, T, H)\n",
    "\n",
    "        # Concatenate prefix + tokens along sequence dimension\n",
    "        inputs_embeds = torch.cat([prefix, token_embeds], dim=1)  # (B, prefix_len + T, H)\n",
    "\n",
    "        # Build attention mask, padding ones for the prefix\n",
    "        if attention_mask is not None:\n",
    "            prefix_mask = torch.ones(\n",
    "                (B, self.prefix_len), dtype=attention_mask.dtype, device=attention_mask.device\n",
    "            )\n",
    "            attention_mask_full = torch.cat([prefix_mask, attention_mask], dim=1)\n",
    "        else:\n",
    "            attention_mask_full = None\n",
    "\n",
    "        # Build labels, ignoring prefix positions with -100\n",
    "        if labels is not None:\n",
    "            prefix_labels = torch.full(\n",
    "                (B, self.prefix_len), -100, dtype=labels.dtype, device=labels.device\n",
    "            )\n",
    "            labels_full = torch.cat([prefix_labels, labels], dim=1)\n",
    "        else:\n",
    "            labels_full = None\n",
    "\n",
    "        outputs = self.gpt2(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask_full,\n",
    "            labels=labels_full,\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "prefix_len = 10  # you can tune this\n",
    "model = PointBertClipCap3D(point_encoder, gpt2, prefix_len=prefix_len).to(device)\n",
    "\n",
    "print(\"Model ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc91115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          id  \\\n",
      "0  03001627_5ceabffee1c333293002761e7a3ba3bd   \n",
      "1   04090263_4ce26b6d23caecb3cc34b900bb2492e   \n",
      "2  03001627_7178731312819be3ecb14096838a20c5   \n",
      "3  03691459_91b781b40d32b74dc491effd0ae881ea   \n",
      "4  04401088_c3b9cb70c6a80ed686a04ec9e4169973   \n",
      "\n",
      "                                             caption  \n",
      "0  A modern office chair featuring a plush green ...  \n",
      "1  A modern tactical rifle featuring an ergonomic...  \n",
      "2  A modern chair featuring a sleek design with a...  \n",
      "3  A rectangular storage unit featuring a promine...  \n",
      "4  A sleek, rectangular smartphone featuring a gl...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52472, torch.Size([32, 16384, 6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset.dataset import Cap3DShapeNetPreprocessed\n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.Tensor, str]]):\n",
    "    \"\"\"\n",
    "    Collate function to:\n",
    "      - stack point clouds\n",
    "      - tokenize captions\n",
    "    \"\"\"\n",
    "    pts_list, captions = zip(*batch)\n",
    "\n",
    "    # Stack point clouds -> (B, N, 6)\n",
    "    pts_batch = torch.stack(pts_list, dim=0).float()\n",
    "\n",
    "    # Tokenize captions\n",
    "    enc = tokenizer(\n",
    "        list(captions),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=32,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Use input_ids as labels (standard LM training)\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    return pts_batch, input_ids, attention_mask, labels, captions\n",
    "\n",
    "dataset = Cap3DShapeNetPreprocessed(\n",
    "    points_path=\"dataset/data/shapenet/processed_points.pt\",\n",
    "    ids_path=\"dataset/data/shapenet/point_ids.json\",\n",
    "    csv_path=\"dataset/data/shapenet/Cap3D_automated_ShapeNet.csv\",\n",
    "    device =device,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "len(dataset), next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa19c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 frozen (only mapper + point encoder will train).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "freeze_gpt2 = True  # set False if you want to fine-tune GPT-2\n",
    "\n",
    "if freeze_gpt2:\n",
    "    for param in model.gpt2.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"GPT-2 frozen (only mapper will train).\")\n",
    "else:\n",
    "    print(\"GPT-2 will be fine-tuned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5624924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
      "100%|██████████| 1640/1640 [16:25<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Avg loss: 3.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [16:15<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Avg loss: 2.7640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [16:13<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Avg loss: 2.7092\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "lr = 1e-4\n",
    "\n",
    "# Only train the parts that require grad\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=lr)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for step, (pts, input_ids, attention_mask, labels, raw_caps) in enumerate(tqdm(train_loader)):\n",
    "        pts = pts.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            pts=pts,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # if (step + 1) % 20 == 0:\n",
    "        #     avg_loss = total_loss / (step + 1)\n",
    "        #     print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} finished. Avg loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"checkpoints/test_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cb8af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth caption: A solid rectangular block with a rich mahogany finish, featuring a smooth surface that exhibits subtle wood grain patterns. Accompanying this are broader structures with multiple circular handles, also finished in mahogany, offering a contemporary design suitable for storage solutions. The arrangement showcases a cohesive aesthetic with a warm, inviting color palette.\n",
      "Generated caption: The sleek, modern wooden cabinet features a rich mahogany finish with a rich mahogany finish\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate_caption_from_points(\n",
    "    model: PointBertClipCap3D,\n",
    "    tokenizer: GPT2TokenizerFast,\n",
    "    pts: torch.Tensor,\n",
    "    max_new_tokens: int = 30,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    pts: (N, C) tensor on CPU\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    pts = pts.unsqueeze(0).to(device)  # (1, N, C)\n",
    "    prefix = model.encode_prefix(pts)  # (1, prefix_len, H)\n",
    "\n",
    "    # Start with BOS token\n",
    "    bos_id = tokenizer.bos_token_id or tokenizer.eos_token_id\n",
    "    generated = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Token embeddings for current generated sequence\n",
    "        token_embeds = model.gpt2.transformer.wte(generated)  # (1, t, H)\n",
    "\n",
    "        # Concatenate prefix + tokens\n",
    "        inputs_embeds = torch.cat([prefix, token_embeds], dim=1)  # (1, prefix_len + t, H)\n",
    "\n",
    "        outputs = model.gpt2(inputs_embeds=inputs_embeds)\n",
    "        next_token_logits = outputs.logits[:, -1, :]  # (1, vocab)\n",
    "\n",
    "        # Optionally apply temperature & top-k\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "        if top_k > 0:\n",
    "            values, indices = torch.topk(next_token_logits, top_k)\n",
    "            probs = torch.softmax(values, dim=-1)\n",
    "            next_token = indices[0, torch.multinomial(probs[0], num_samples=1)]\n",
    "            next_token = next_token.unsqueeze(0).unsqueeze(0)\n",
    "        else:\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Drop BOS, decode\n",
    "    caption_ids = generated[0, 1:]\n",
    "    caption = tokenizer.decode(caption_ids, skip_special_tokens=True)\n",
    "    return caption.strip()\n",
    "\n",
    "\n",
    "# Test on a random sample from the dummy dataset\n",
    "test_pts, test_caption = dataset[10000]\n",
    "print(\"Ground truth caption:\", test_caption)\n",
    "\n",
    "gen_caption = generate_caption_from_points(model, tokenizer, test_pts, max_new_tokens=20)\n",
    "print(\"Generated caption:\", gen_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
